<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Primary metadata -->
  <meta charset="utf-8">
  <meta name="author" content="Prasun Roy">
  <meta name="description" content="PhD Student @ UTS, University of Technology Sydney">
  <meta name="keywords" content="Prasun Roy, Personal Website">
  <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->
  <!-- Title -->
  <title>Prasun Roy</title>
  <!-- External resource links -->
  <link rel="icon" type="image/x-icon" href="static/favicon.ico">
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Rubik&display=swap">
  <!-- Custom styles -->
  <style type="text/css">
    body {font-family: "Rubik", Verdana, Helvetica, sans-serif; font-size: 14px; text-align: justify;}
    a {color: #4040e0; text-decoration: none;}
    a:focus, a:hover {background-color: #ffffcc; color: #ff4040; text-decoration: none;}
    .pr-outer-table {width: 100%; max-width: 800px; border: 0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;}
    .pr-inner-table {width: 100%; border: 0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;}
    .pr-profile-bio-container {padding: 2.5%; width: 70%; vertical-align: middle;}
    .pr-profile-img-container {padding: 2.5%; width: 30%; max-width: 30%;}
    .pr-profile-img {width: 100%; max-width: 100%; object-fit: cover; border-radius: 10%;}
    .pr-profile-name {padding-top: 20px; margin: 0; font-size: 32px;}
    .pr-research-container {padding: 16px; width: 100%; vertical-align: middle;}
    .pr-research-header-l-container {padding-left: 16px; width: 20%; vertical-align: middle;}
    .pr-research-header-r-container {padding-left: 0px; width: 80%; vertical-align: middle;}
    .pr-research-info-container {padding: 8px; width: 80%; vertical-align:middle;}
    .pr-research-logo-container {padding: 16px; width: 20%; vertical-align: middle;}
    .pr-research-logo-img {width: 160px; height: 160px; position: relative;}
    .pr-research-logo-vid {width: 160px; height: 160px; position: absolute; transition: opacity .2s ease-in-out; -moz-transition: opacity .2s ease-in-out; -webkit-transition: opacity .2s ease-in-out;}
    .pr-research-title {font-weight: 800;}
    .pr-research-venue {padding-top: 8px; padding-bottom: 8px;}
    .pr-research-venue-oral {color: #ff4040;}
    .pr-text-center {text-align: center;}
    .pr-zero-padding {padding: 0px;}
    .pr-copyright {font-size: small; text-align: right;}
    .pr-highlight {background-color: #e0ffcc;}
    .pr-inactive {color: #404040; pointer-events: none;}
    .pr-blink {animation: blinker 1.0s linear infinite;}
    @keyframes blinker {50% {opacity: 0;}}
  </style>
</head>

<body>
  <table class="pr-outer-table">
    <tbody>
      <tr class="pr-zero-padding">
        <td class="pr-zero-padding">
          <!-- Profile section -->
          <table class="pr-inner-table">
            <tbody>
              <tr class="pr-zero-padding">
                <td class="pr-profile-bio-container">
                  <p class="pr-profile-name pr-text-center">
                    Prasun Roy
                  </p>
                  <p>
                    I am a PhD student at the School of Computer Science, <a href="https://www.uts.edu.au">University of Technology Sydney</a>, Australia, advised by <a href="https://profiles.uts.edu.au/Michael.Blumenstein">Prof Michael Blumenstein</a> and <a href="https://www.isical.ac.in/~umapada">Prof Umapada Pal</a>. I am a member of the <a href="https://www.uts.edu.au/about/faculty-engineering-and-information-technology/global-engagement/india/indian-statistical-institute">ISI-UTS Joint Research Cluster</a> and the <a href="https://www.uts.edu.au/research/australian-artificial-intelligence-institute">Australian Artificial Intelligence Institute (AAII)</a>.
                  </p>
                  <p>
                    Previously, I was a Research Associate at the Computer Vision and Pattern Recognition Unit, <a href="https://www.isical.ac.in">Indian Statistical Institute, Kolkata</a>.
                  </p>
                  <p class="pr-text-center">
                    <a href="mailto:prasunroy.pr@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="static/data/profile/PrasunRoy_CV.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="static/data/profile/PrasunRoy_Bio.txt">Bio</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=n6T5cSsAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/prasunroy">GitHub</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/prasunroy">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/_prasunroy">Twitter</a>
                  </p>
                </td>
                <td class="pr-profile-img-container">
                  <img class="pr-profile-img" alt="profile photo" src="static/data/profile/PrasunRoy.jpg">
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Research section -->
          <table class="pr-inner-table">
            <tbody>
              <tr class="pr-zero-padding">
                <td class="pr-research-container">
                  <h2>Research</h2>
                  <p>
                    I am interested in computer vision, deep learning, generative models, image processing, graphics, and applied machine learning. Most of my recent research focuses on text style manipulation and human pose transformation. Some publications are <span class="pr-highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table class="pr-inner-table">
            <!-- Research Spotlights subsection -->
            <tbody>
              <tr class="pr-zero-padding">
                <td class="pr-research-header-l-container">
                  <span class="pr-research-title">> Research Spotlights <span class="pr-blink">_</span></span>
                </td>
                <td class="pr-research-header-r-container">
                  <span></span>
                </td>
              </tr>
            </tbody>
            <tbody>
              <!------------------------------------------------------------------------------
              -- TIPS: Text-Induced Pose Synthesis --
              ------------------------------------------------------------------------------->
              <tr onmouseover="tips_start()" onmouseout="tips_stop()">
                <td class="pr-research-logo-container">
                  <div class="pr-research-logo-img">
                    <div class="pr-research-logo-vid" id="tips-logo">
                      <video width=100% muted autoplay loop>
                        <!-- <source type="video/mp4" src="static/data/research/roy2022tips/teaser.mp4"> -->
                      </video>
                    </div>
                    <img width=100% src="static/data/research/roy2022tips/teaser.jpg">
                  </div>
                  <script type="text/javascript">
                    function tips_start() {
                      document.getElementById("tips-logo").style.opacity = "1";
                    }
                    function tips_stop() {
                      document.getElementById("tips-logo").style.opacity = "0";
                    }
                    tips_stop();
                  </script>
                </td>
                <td class="pr-research-info-container">
                  <a href="https://prasunroy.github.io/tips"><span class="pr-research-title">TIPS: Text-Induced Pose Synthesis</span></a>
                  <br>
                  <strong>Prasun Roy</strong>,
                  <a href="https://scholar.google.com/citations?user=vTSn-xkAAAAJ&hl=en">Subhankar Ghosh</a>,
                  <a href="https://scholar.google.com/citations?user=8pffuA4AAAAJ&hl=en">Saumik Bhattacharya</a>,
                  <a href="https://scholar.google.com/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>,
                  <a href="https://scholar.google.com/citations?user=UigwQR0AAAAJ&hl=en">Michael Blumenstein</a>
                  <div class="pr-research-venue">
                    <em>ECCV</em>, 2022
                  </div>
                  <a href="https://prasunroy.github.io/tips">Project Page</a> &nbsp;/&nbsp;
                  <a href="https://github.com/prasunroy/tips">Code</a> &nbsp;/&nbsp;
                  <a href="http://arxiv.org/abs/2207.11718">arXiv</a> &nbsp;/&nbsp;
                  <a href="static/data/research/roy2022tips/bibtex.bib">BibTex</a>
                  <p>
                    We address the structural bias in pose-guided person image generation techniques with a text-conditioned human pose transformation strategy.
                  </p>
                </td>
              </tr>
              <!------------------------------------------------------------------------------
              -- STEFANN: Scene Text Editor using Font Adaptive Neural Network --
              ------------------------------------------------------------------------------->
              <tr class="pr-highlight" onmouseover="stefann_start()" onmouseout="stefann_stop()">
                <td class="pr-research-logo-container">
                  <div class="pr-research-logo-img">
                    <div class="pr-research-logo-vid" id="stefann-logo">
                      <video width=100% muted autoplay loop>
                        <!-- <source type="video/mp4" src="static/data/research/roy2020stefann/teaser.mp4"> -->
                      </video>
                    </div>
                    <img width=100% src="static/data/research/roy2020stefann/teaser.jpg">
                  </div>
                  <script type="text/javascript">
                    function stefann_start() {
                      document.getElementById("stefann-logo").style.opacity = "1";
                    }
                    function stefann_stop() {
                      document.getElementById("stefann-logo").style.opacity = "0";
                    }
                    stefann_stop();
                  </script>
                </td>
                <td class="pr-research-info-container">
                  <a href="https://prasunroy.github.io/stefann"><span class="pr-research-title">STEFANN: Scene Text Editor using Font Adaptive Neural Network</span></a>
                  <br>
                  <strong>Prasun Roy</strong>,
                  <a href="https://scholar.google.com/citations?user=8pffuA4AAAAJ&hl=en">Saumik Bhattacharya</a>,
                  <a href="https://scholar.google.com/citations?user=vTSn-xkAAAAJ&hl=en">Subhankar Ghosh</a>,
                  <a href="https://scholar.google.com/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>
                  <div class="pr-research-venue">
                    <em>CVPR</em>, 2020
                  </div>
                  <a href="https://prasunroy.github.io/stefann">Project Page</a> &nbsp;/&nbsp;
                  <a href="https://github.com/prasunroy/stefann">Code</a> &nbsp;/&nbsp;
                  <a href="https://arxiv.org/abs/1903.01192">arXiv</a> &nbsp;/&nbsp;
                  <a href="static/data/research/roy2020stefann/bibtex.bib">BibTex</a>
                  <p>
                    We introduce a technique for character-level realistic text modification in a scene by disentangling the task into dedicated shape and color transformation objectives.
                  </p>
                </td>
              </tr>
            </tbody>
            <!-- Selected Publications subsection -->
            <tbody>
              <tr class="pr-zero-padding">
                <td class="pr-research-header-l-container">
                  <br>
                  <span class="pr-research-title">> Selected Publications <span class="pr-blink">_</span></span>
                </td>
                <td class="pr-research-header-r-container">
                  <span></span>
                </td>
              </tr>
            </tbody>
            <tbody>
              <!------------------------------------------------------------------------------
              -- Exploring Mutual Cross-Modal Attention for Context-Aware Human Affordance
                 Generation --
              ------------------------------------------------------------------------------->
              <tr onmouseover="mcma_start()" onmouseout="mcma_stop()">
                <td class="pr-research-logo-container">
                  <div class="pr-research-logo-img">
                    <div class="pr-research-logo-vid" id="mcma-logo">
                      <video width=100% muted autoplay loop>
                        <!-- <source type="video/mp4" src="static/data/research/roy2025exploring/teaser.mp4"> -->
                      </video>
                    </div>
                    <img width=100% src="static/data/research/roy2025exploring/teaser.jpg">
                  </div>
                  <script type="text/javascript">
                    function mcma_start() {
                      document.getElementById("mcma-logo").style.opacity = "1";
                    }
                    function mcma_stop() {
                      document.getElementById("mcma-logo").style.opacity = "0";
                    }
                    mcma_stop();
                  </script>
                </td>
                <td class="pr-research-info-container">
                  <a href="https://arxiv.org/abs/2502.13637"><span class="pr-research-title">Exploring Mutual Cross-Modal Attention for Context-Aware Human Affordance Generation</span></a>
                  <br>
                  <strong>Prasun Roy</strong>,
                  <a href="https://scholar.google.com/citations?user=8pffuA4AAAAJ&hl=en">Saumik Bhattacharya</a>,
                  <a href="https://scholar.google.com/citations?user=vTSn-xkAAAAJ&hl=en">Subhankar Ghosh</a>,
                  <a href="https://scholar.google.com/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>,
                  <a href="https://scholar.google.com/citations?user=UigwQR0AAAAJ&hl=en">Michael Blumenstein</a>
                  <div class="pr-research-venue">
                    <em>arXiv</em>, 2025
                  </div>
                  <a class="pr-inactive" href="">Project Page</a> &nbsp;/&nbsp;
                  <a href="https://github.com/prasunroy/mcma">Code</a> &nbsp;/&nbsp;
                  <a href="https://arxiv.org/abs/2502.13637">arXiv</a> &nbsp;/&nbsp;
                  <a href="static/data/research/roy2025exploring/bibtex.bib">BibTex</a>
                  <p>
                    By mutually cross-attending two different spatial feature spaces, we encode the global scene context for semantically meaningful affordance generation.
                  </p>
                </td>
              </tr>
              <!------------------------------------------------------------------------------
              -- FASTER: A Font-Agnostic Scene Text Editing and Rendering Framework --
              ------------------------------------------------------------------------------->
              <tr onmouseover="faster_start()" onmouseout="faster_stop()">
                <td class="pr-research-logo-container">
                  <div class="pr-research-logo-img">
                    <div class="pr-research-logo-vid" id="faster-logo">
                      <video width=100% muted autoplay loop>
                        <!-- <source type="video/mp4" src="static/data/research/das2025faster/teaser.mp4"> -->
                      </video>
                    </div>
                    <img width=100% src="static/data/research/das2025faster/teaser.jpg">
                  </div>
                  <script type="text/javascript">
                    function faster_start() {
                      document.getElementById("faster-logo").style.opacity = "1";
                    }
                    function faster_stop() {
                      document.getElementById("faster-logo").style.opacity = "0";
                    }
                    faster_stop();
                  </script>
                </td>
                <td class="pr-research-info-container">
                  <a href="https://arxiv.org/abs/2308.02905"><span class="pr-research-title">FASTER: A Font-Agnostic Scene Text Editing and Rendering Framework</span></a>
                  <br>
                  <a href="https://alloydas.github.io">Alloy Das</a>,
                  <a href="https://scholar.google.com/citations?user=J-VlCNYAAAAJ&hl=en">Sanket Biswas</a>,
                  <strong>Prasun Roy</strong>,
                  <a href="https://scholar.google.com/citations?user=vTSn-xkAAAAJ&hl=en">Subhankar Ghosh</a>,
                  <a href="https://scholar.google.com/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>,
                  <a href="https://scholar.google.com/citations?user=UigwQR0AAAAJ&hl=en">Michael Blumenstein</a>,
                  <a href="https://scholar.google.com/citations?user=92pWl-AAAAAJ&hl=en">Josep Lladós</a>,
                  <a href="https://scholar.google.com/citations?user=8pffuA4AAAAJ&hl=en">Saumik Bhattacharya</a>
                  <div class="pr-research-venue">
                    <em>WACV</em>, 2025&nbsp;&nbsp;<span class="pr-research-venue-oral">(Oral presentation)</span>
                  </div>
                  <a class="pr-inactive" href="">Project Page</a> &nbsp;/&nbsp;
                  <a class="pr-inactive" href="">Code</a> &nbsp;/&nbsp;
                  <a href="https://arxiv.org/abs/2308.02905">arXiv</a> &nbsp;/&nbsp;
                  <a href="static/data/research/das2025faster/bibtex.bib">BibTex</a>
                  <p>
                    By adopting a cascaded attention mechanism, we perform word-level style and content translation for realistic text manipulation in a scene.
                  </p>
                </td>
              </tr>
              <!------------------------------------------------------------------------------
              -- Semantically Consistent Person Image Generation --
              ------------------------------------------------------------------------------->
              <tr onmouseover="scpig_start()" onmouseout="scpig_stop()">
                <td class="pr-research-logo-container">
                  <div class="pr-research-logo-img">
                    <div class="pr-research-logo-vid" id="scpig-logo">
                      <video width=100% muted autoplay loop>
                        <!-- <source type="video/mp4" src="static/data/research/roy2024semantically/teaser.mp4"> -->
                      </video>
                    </div>
                    <img width=100% src="static/data/research/roy2024semantically/teaser.jpg">
                  </div>
                  <script type="text/javascript">
                    function scpig_start() {
                      document.getElementById("scpig-logo").style.opacity = "1";
                    }
                    function scpig_stop() {
                      document.getElementById("scpig-logo").style.opacity = "0";
                    }
                    scpig_stop();
                  </script>
                </td>
                <td class="pr-research-info-container">
                  <a href="https://arxiv.org/abs/2302.14728"><span class="pr-research-title">Semantically Consistent Person Image Generation</span></a>
                  <br>
                  <strong>Prasun Roy</strong>,
                  <a href="https://scholar.google.com/citations?user=8pffuA4AAAAJ&hl=en">Saumik Bhattacharya</a>,
                  <a href="https://scholar.google.com/citations?user=vTSn-xkAAAAJ&hl=en">Subhankar Ghosh</a>,
                  <a href="https://scholar.google.com/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>,
                  <a href="https://scholar.google.com/citations?user=UigwQR0AAAAJ&hl=en">Michael Blumenstein</a>
                  <div class="pr-research-venue">
                    <em>ICPR</em>, 2024
                  </div>
                  <a class="pr-inactive" href="">Project Page</a> &nbsp;/&nbsp;
                  <a class="pr-inactive" href="">Code</a> &nbsp;/&nbsp;
                  <a href="https://arxiv.org/abs/2302.14728">arXiv</a> &nbsp;/&nbsp;
                  <a href="static/data/research/roy2025exploring/bibtex.bib">BibTex</a>
                  <p>
                    Using a parsing map-based representation, we propose a method for introducing a new person into a scene such that the inserted person is semantically consistent with the existing individuals.
                  </p>
                </td>
              </tr>
              <!------------------------------------------------------------------------------
              -- d-Sketch: Improving Visual Fidelity of Sketch-to-Image Translation with
                 Pretrained Latent Diffusion Models without Retraining --
              ------------------------------------------------------------------------------->
              <tr onmouseover="dsketch_start()" onmouseout="dsketch_stop()">
                <td class="pr-research-logo-container">
                  <div class="pr-research-logo-img">
                    <div class="pr-research-logo-vid" id="dsketch-logo">
                      <video width=100% muted autoplay loop>
                        <!-- <source type="video/mp4" src="static/data/research/roy2024dsketch/teaser.mp4"> -->
                      </video>
                    </div>
                    <img width=100% src="static/data/research/roy2024dsketch/teaser.jpg">
                  </div>
                  <script type="text/javascript">
                    function dsketch_start() {
                      document.getElementById("dsketch-logo").style.opacity = "1";
                    }
                    function dsketch_stop() {
                      document.getElementById("dsketch-logo").style.opacity = "0";
                    }
                    dsketch_stop();
                  </script>
                </td>
                <td class="pr-research-info-container">
                  <a href="https://arxiv.org/abs/2502.14007"><span class="pr-research-title">d-Sketch: Improving Visual Fidelity of Sketch-to-Image Translation with Pretrained Latent Diffusion Models without Retraining</span></a>
                  <br>
                  <strong>Prasun Roy</strong>,
                  <a href="https://scholar.google.com/citations?user=8pffuA4AAAAJ&hl=en">Saumik Bhattacharya</a>,
                  <a href="https://scholar.google.com/citations?user=vTSn-xkAAAAJ&hl=en">Subhankar Ghosh</a>,
                  <a href="https://scholar.google.com/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>,
                  <a href="https://scholar.google.com/citations?user=UigwQR0AAAAJ&hl=en">Michael Blumenstein</a>
                  <div class="pr-research-venue">
                    <em>ICPR</em>, 2024
                  </div>
                  <a class="pr-inactive" href="">Project Page</a> &nbsp;/&nbsp;
                  <a href="https://github.com/prasunroy/dsketch">Code</a> &nbsp;/&nbsp;
                  <a href="https://arxiv.org/abs/2502.14007">arXiv</a> &nbsp;/&nbsp;
                  <a href="static/data/research/roy2024dsketch/bibtex.bib">BibTex</a>
                  <p>
                    A small trainable latent mapping network lets you perform photorealistic sketch-to-image translation using a pretrained text-to-image diffusion model without retraining.
                  </p>
                </td>
              </tr>
              <!------------------------------------------------------------------------------
              -- Multi-scale Attention Guided Pose Transfer --
              ------------------------------------------------------------------------------->
              <tr onmouseover="msagpt_start()" onmouseout="msagpt_stop()">
                <td class="pr-research-logo-container">
                  <div class="pr-research-logo-img">
                    <div class="pr-research-logo-vid" id="msagpt-logo">
                      <video width=100% muted autoplay loop>
                        <!-- <source type="video/mp4" src="static/data/research/roy2023multi/teaser.mp4"> -->
                      </video>
                    </div>
                    <img width=100% src="static/data/research/roy2023multi/teaser.jpg">
                  </div>
                  <script type="text/javascript">
                    function msagpt_start() {
                      document.getElementById("msagpt-logo").style.opacity = "1";
                    }
                    function msagpt_stop() {
                      document.getElementById("msagpt-logo").style.opacity = "0";
                    }
                    msagpt_stop();
                  </script>
                </td>
                <td class="pr-research-info-container">
                  <a href="https://arxiv.org/abs/2202.06777"><span class="pr-research-title">Multi-scale Attention Guided Pose Transfer</span></a>
                  <br>
                  <strong>Prasun Roy</strong>,
                  <a href="https://scholar.google.com/citations?user=8pffuA4AAAAJ&hl=en">Saumik Bhattacharya</a>,
                  <a href="https://scholar.google.com/citations?user=vTSn-xkAAAAJ&hl=en">Subhankar Ghosh</a>,
                  <a href="https://scholar.google.com/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>
                  <div class="pr-research-venue">
                    <em>Pattern Recognition</em>, 2023
                  </div>
                  <a class="pr-inactive" href="">Project Page</a> &nbsp;/&nbsp;
                  <a href="https://github.com/prasunroy/pose-transfer">Code</a> &nbsp;/&nbsp;
                  <a href="https://arxiv.org/abs/2202.06777">arXiv</a> &nbsp;/&nbsp;
                  <a href="static/data/research/roy2023multi/bibtex.bib">BibTex</a>
                  <p>
                    Cascaded attention at every feature resolution improves the generated image quality by retaining both low-frequency and high-frequency visual attributes in a structurally guided end-to-end human pose transformation.
                  </p>
                </td>
              </tr>
              <!------------------------------------------------------------------------------
              -- Scene Aware Person Image Generation through Global Contextual Conditioning --
              ------------------------------------------------------------------------------->
              <tr onmouseover="sapig_start()" onmouseout="sapig_stop()">
                <td class="pr-research-logo-container">
                  <div class="pr-research-logo-img">
                    <div class="pr-research-logo-vid" id="sapig-logo">
                      <video width=100% muted autoplay loop>
                        <!-- <source type="video/mp4" src="static/data/research/roy2022scene/teaser.mp4"> -->
                      </video>
                    </div>
                    <img width=100% src="static/data/research/roy2022scene/teaser.jpg">
                  </div>
                  <script type="text/javascript">
                    function sapig_start() {
                      document.getElementById("sapig-logo").style.opacity = "1";
                    }
                    function sapig_stop() {
                      document.getElementById("sapig-logo").style.opacity = "0";
                    }
                    sapig_stop();
                  </script>
                </td>
                <td class="pr-research-info-container">
                  <a href="https://arxiv.org/abs/2206.02717"><span class="pr-research-title">Scene Aware Person Image Generation through Global Contextual Conditioning</span></a>
                  <br>
                  <strong>Prasun Roy</strong>,
                  <a href="https://scholar.google.com/citations?user=vTSn-xkAAAAJ&hl=en">Subhankar Ghosh</a>,
                  <a href="https://scholar.google.com/citations?user=8pffuA4AAAAJ&hl=en">Saumik Bhattacharya</a>,
                  <a href="https://scholar.google.com/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>,
                  <a href="https://scholar.google.com/citations?user=UigwQR0AAAAJ&hl=en">Michael Blumenstein</a>
                  <div class="pr-research-venue">
                    <em>ICPR</em>, 2022
                  </div>
                  <a class="pr-inactive" href="">Project Page</a> &nbsp;/&nbsp;
                  <a class="pr-inactive" href="">Code</a> &nbsp;/&nbsp;
                  <a href="https://arxiv.org/abs/2206.02717">arXiv</a> &nbsp;/&nbsp;
                  <a href="static/data/research/roy2022scene/bibtex.bib">BibTex</a>
                  <p>
                    Using a keypoint-based representation, we propose a method for introducing a new person into a scene such that the inserted person is semantically consistent with the existing individuals.
                  </p>
                </td>
              </tr>
              <!------------------------------------------------------------------------------
              -- Effects of Degradations on Deep Neural Network Architectures --
              ------------------------------------------------------------------------------->
              <tr onmouseover="eddnna_start()" onmouseout="eddnna_stop()">
                <td class="pr-research-logo-container">
                  <div class="pr-research-logo-img">
                    <div class="pr-research-logo-vid" id="eddnna-logo">
                      <video width=100% muted autoplay loop>
                        <!-- <source type="video/mp4" src="static/data/research/roy2018effects/teaser.mp4"> -->
                      </video>
                    </div>
                    <img width=100% src="static/data/research/roy2018effects/teaser.jpg">
                  </div>
                  <script type="text/javascript">
                    function eddnna_start() {
                      document.getElementById("eddnna-logo").style.opacity = "1";
                    }
                    function eddnna_stop() {
                      document.getElementById("eddnna-logo").style.opacity = "0";
                    }
                    eddnna_stop();
                  </script>
                </td>
                <td class="pr-research-info-container">
                  <a href="https://arxiv.org/abs/1807.10108"><span class="pr-research-title">Effects of Degradations on Deep Neural Network Architectures</span></a>
                  <br>
                  <strong>Prasun Roy</strong>,
                  <a href="https://scholar.google.com/citations?user=vTSn-xkAAAAJ&hl=en">Subhankar Ghosh</a>,
                  <a href="https://scholar.google.com/citations?user=8pffuA4AAAAJ&hl=en">Saumik Bhattacharya</a>,
                  <a href="https://scholar.google.com/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>
                  <div class="pr-research-venue">
                    <em>arXiv</em>, 2018
                  </div>
                  <a class="pr-inactive" href="">Project Page</a> &nbsp;/&nbsp;
                  <a href="https://github.com/prasunroy/cnn-on-degraded-images">Code</a> &nbsp;/&nbsp;
                  <a href="https://arxiv.org/abs/1807.10108">arXiv</a> &nbsp;/&nbsp;
                  <a href="static/data/research/roy2018effects/bibtex.bib">BibTex</a>
                  <p>
                    A study on how different image degradation models impact the performance decay of deep neural networks unveils fascinating insights for substantially improving noise tolerance at the expense of slight performance trade-offs.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Copyright section -->
          <table class="pr-inner-table">
            <tbody>
              <tr class="pr-zero-padding">
                <td class="pr-zero-padding">
                  <p class="pr-copyright">
                    <br>
                    Yes. I'm also using <a href="https://jonbarron.info">Jon Barron</a>'s website template.😅
                    <br>
                    Copyright &#169; 2025 Prasun Roy.✨
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>







